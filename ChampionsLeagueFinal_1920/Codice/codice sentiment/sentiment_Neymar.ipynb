{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "from pymongo import MongoClient\n",
    "\n",
    "client = MongoClient('mongo', 27017, username='admin', password='DataMan2019!')\n",
    "\n",
    "db=client.UCLFinal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg=db.pre_partita.aggregate([\n",
    "   {'$match':{\"$and\":[{\"timestamp\":{\"$gt\":\"2020-08-23 19:24:00\",\"$lte\":\"2020-08-23 21:00:00\"}}, \n",
    "    {\"text\":{'$regex': \"[Nn]eymar\"}}]}},  \n",
    "     {\n",
    "       '$project':\n",
    "         {\n",
    "             'id':'$id',\n",
    "             \n",
    "             'language':'$language',\n",
    "             'timestamp':'$timestamp',\n",
    "             'text':'$text',\n",
    "             'hashtags':'$hashtags',\n",
    "             \n",
    "             'geo':'$geo',\n",
    "             'coordinates':'$coordinates',\n",
    "             'retweet_count':'$retweet_count',\n",
    "             'retweeted':'$retweeted',\n",
    "             'is_a_retweet':'$is_a_retweet'\n",
    "             \n",
    "            }\n",
    "     },\n",
    "       { '$out' :'Ney_pre'}   ]   \n",
    "   \n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg=db.post_partita.aggregate([\n",
    "   {'$match':{\"$and\":[{\"timestamp\":{\"$gt\":\"2020-08-23 22:54:00\",\"$lte\":\"2020-08-24 00:30:00\"}},\n",
    "    {\"text\":{'$regex': \"[Nn]eymar\"}}]}},  \n",
    "     {\n",
    "       '$project':\n",
    "         {\n",
    "             'id':'$id',\n",
    "             'user_location':'$user_location',\n",
    "             'language':'$language',\n",
    "             'timestamp':'$timestamp',\n",
    "             'text':'$text',\n",
    "             'hashtags':'$hashtags',\n",
    "             \n",
    "             'geo':'$geo',\n",
    "             'coordinates':'$coordinates',\n",
    "             'retweet_count':'$retweet_count',\n",
    "             'retweeted':'$retweeted',\n",
    "             'is_a_retweet':'$is_a_retweet',\n",
    "            \n",
    "            }\n",
    "     },\n",
    "       { '$out' :'Ney_post'}   ]   \n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos=[]\n",
    "neu=[]\n",
    "neg=[]\n",
    "punteggio=[]\n",
    "stdev=[]\n",
    "lb=[]\n",
    "ub=[]\n",
    "stdev=[]\n",
    "confint_score=[]\n",
    "\n",
    "\n",
    " #si creano queste liste vuote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTIMENT ANALYSIS:\n",
    "\n",
    "collezione = db.Ney_post.find({\"language\":\"en\"})   #inseriamo tutti e due i subset e compiliamo i dataset \n",
    "\n",
    "import re\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "positive =0 \n",
    "negative =0\n",
    "neutral=0\n",
    "totale=0\n",
    "\n",
    "lista=[]\n",
    "\n",
    "def clean_tweet(tweet): \n",
    "    ''' Utility function to clean tweet text by removing links, special characters using simple regex statements. ''' \n",
    "    return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+://\\S+)\", \" \", tweet).split())\n",
    "\n",
    "def get_tweet_sentiment(tweet):\n",
    "\n",
    "    global positive\n",
    "    global negative\n",
    "    global neutral\n",
    "    global totale\n",
    "    ''' \n",
    "    Utility function to classify sentiment of passed tweet \n",
    "    using textblob's sentiment method \n",
    "    '''\n",
    "    # create TextBlob object of passed tweet text \n",
    "    analysis = TextBlob(clean_tweet(tweet)) \n",
    "    # set sentiment \n",
    "    #print(analysis.sentiment.polarity)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        lista.append(analysis.sentiment.polarity)\n",
    "        positive=positive+1\n",
    "        #print(analysis)\n",
    "        totale=totale+analysis.sentiment.polarity\n",
    "        return 'positive'\n",
    "\n",
    "    elif analysis.sentiment.polarity == 0: \n",
    "        neutral=neutral+1\n",
    "        #print(analysis)\n",
    "        return 'neutral'\n",
    "    else: \n",
    "        lista.append(analysis.sentiment.polarity)\n",
    "        negative=negative+1\n",
    "        totale=totale+analysis.sentiment.polarity\n",
    "        #print(analysis)\n",
    "        return 'negative'\n",
    "\n",
    "for tweet in collezione:\n",
    "\n",
    "    get_tweet_sentiment(tweet['text'])\n",
    "    \n",
    "\n",
    "#creo 4 liste da comporre in un dataset finale:\n",
    "\n",
    "punt=totale/(positive+negative)\n",
    "dev=math.sqrt(np.var(lista))\n",
    "lower=punt-1.64*dev/(math.sqrt(positive+negative)) #intervallo normale\n",
    "upper=punt+1.64*dev/(math.sqrt(positive+negative))\n",
    "confint=(round(lower,3),round(upper,3))\n",
    "\n",
    "pos.append(positive)\n",
    "neu.append(neutral)\n",
    "neg.append(negative)\n",
    "punteggio.append(punt)\n",
    "lb.append(lower)\n",
    "ub.append(upper)\n",
    "stdev.append(dev)\n",
    "confint_score.append(confint)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ney_pre=pd.DataFrame({'time':'prepartita','positivi':pos,'negativi':neg,'neutrali':neu,'punteggio':punteggio,'confint_score':confint_score})\n",
    "Ney_pre[\"sentiment_score\"]=round(Ney_pre[\"punteggio\"],3)\n",
    "Ney_pre=Ney_pre.drop(\"punteggio\",axis='columns')\n",
    "Ney_pre.to_csv(\"Ney_pre\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ney_post=pd.DataFrame({'time':'postpartita','positivi':pos,'negativi':neg,'neutrali':neu,'punteggio':punteggio,'confint_score':confint_score})\n",
    "Ney_post[\"sentiment_score\"]=round(Ney_post[\"punteggio\"],3)\n",
    "Ney_post=Ney_post.drop(\"punteggio\",axis='columns')\n",
    "Ney_post.to_csv(\"Ney_post\",sep='\\t',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WORD CLOUD NEYMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#girare json in csv:\n",
    "\n",
    "pre_df = pd.DataFrame(list(db.Ney_pre.find()))  #csv prepartita\n",
    "\n",
    "post_df = pd.DataFrame(list(db.Ney_post.find()))  #csv postpartita\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "stop.extend(['#UCL','#ChampionsLeague','#PSGBayern','#UCLFinal','#PSGFCB'])\n",
    "#lista di stop word inglesi + hastags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_df[\"text\"]=pre_df[\"text\"].str.lower().str.split()\n",
    "pre_df[\"text\"]=pre_df[\"text\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "pre_df.to_csv(\"Neymar_prepartita_fin.csv\",sep=';')\n",
    "#file csv da aprire su excel e lavorarci in maniera tale da ottenere cloud word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df[\"text\"]=post_df[\"text\"].str.lower().str.split()\n",
    "post_df[\"text\"]=post_df[\"text\"].apply(lambda x: [item for item in x if item not in stop])\n",
    "post_df.to_csv(\"Neymar_postpartita_fin.csv\",sep=';')\n",
    "#file csv postpartita da aprire su excel e creare cloud word post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
